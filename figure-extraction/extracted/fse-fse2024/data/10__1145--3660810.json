[{
  "caption": "Fig. 1. The Overview of ClarifyGPT",
  "captionBoundary": {
    "x1": 175.64500427246094,
    "x2": 310.3561096191406,
    "y1": 333.9738464355469,
    "y2": 338.20599365234375
  },
  "figType": "Figure",
  "imageText": [],
  "name": "1",
  "page": 5,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 84.0,
    "y2": 319.2
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Figure1-1.png"
}, {
  "caption": "Table 2. Statistics of benchmarks: the total number of problems in the benchmark (Problem Nums), the average number of test cases per problem (AVG.Tests per Problem), and the average/maximum/minimum number of prompt words in the benchmark (AVG/MAX/MIN.Words in Prompt).",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 440.1725769042969,
    "y1": 88.12186431884766,
    "y2": 114.27099609375
  },
  "figType": "Table",
  "imageText": ["MIN.Words", "in", "Prompt", "17", "17", "7", "7", "8", "MAX.Words", "in", "Prompt", "249", "249", "47", "47", "103", "AVG.Words", "in", "Prompt", "67.7", "67.7", "14.5", "14.5", "42.84", "AVG.Tests", "per", "Problem", "7.8", "107.5", "3.1", "101.7", "-", "Problem", "Nums", "164", "164", "427", "427", "163", "Benchmark", "HumanEval", "HumanEval-ET", "MBPP-sanitized", "MBPP-ET", "CoderEval"],
  "name": "2",
  "page": 10,
  "regionBoundary": {
    "x1": 86.88,
    "x2": 399.36,
    "y1": 128.64,
    "y2": 201.12
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table2-1.png"
}, {
  "caption": "Table 5. Experimental results of ClarifyGPT with di erent number of demonstrations. Numbers in red denote the relative improvement of ClarifyGPT compared to the Default.",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 440.1706848144531,
    "y1": 88.12186431884766,
    "y2": 103.31298828125
  },
  "figType": "Table",
  "imageText": ["ClarifyGPT", "(zero-shot)", "79.26", "0.5%", "↑", "70.73", "0.0%", "-", "72.13", "1.6%", "↑", "52.22", "1.4%", "↑", "41.10", "2.5%", "↑", "63.09", "1.2%", "↑", "ClarifyGPT", "(one-shot)", "83.93", "6.4%", "↑", "72.76", "2.9%", "↑", "75.88", "6.9%", "↑", "55.97", "8.6%", "↑", "41.92", "4.6%", "↑", "66.09", "5.9%", "↑", "ClarifyGPT", "(two-shot)", "85.15", "8.0%", "↑", "75.61", "6.9%", "↑", "77.75", "9.6%", "↑", "56.67", "10.0%", "↑", "43.56", "8.7%", "↑", "67.75", "8.6%", "↑", "ClarifyGPT", "(three-shot)", "87.80", "11.3%", "↑", "78.05", "10.3%", "↑", "78.69", "10.9%", "↑", "58.4713.5%", "↑", "44.99", "12.3%", "↑", "69.60", "11.7%", "↑", "Default", "78.86", "70.73", "70.96", "51.52", "40.08", "62.43", "GPT-4", "ClarifyGPT", "(zero-shot)", "65.85", "1.9%", "↑", "58.13", "1.4%", "↑", "67.07", "2.3%", "↑", "48.01", "2.8%", "↑", "39.26", "4.9%", "↑", "55.66", "2.7%", "↑", "ClarifyGPT", "(one-shot)", "72.80", "12.6%", "↑", "60.98", "6.4%", "↑", "70.96", "8.2%", "↑", "51.52", "10.4%", "↑", "40.70", "8.8%", "↑", "59.39", "9.3%", "↑", "ClarifyGPT", "(two-shot)", "73.92", "14.4%", "↑", "63.21", "10.3%", "↑", "72.60", "10.7%", "↑", "53.63", "14.9%", "↑", "42.13", "12.6%", "↑", "61.10", "12.6%", "↑", "ClarifyGPT", "(three-shot)", "74.39", "15.1%", "↑", "64.84", "13.1%", "↑", "74.08", "13.0%", "↑", "55.58", "19.1%", "↑", "42.94", "14.8%", "↑", "62.37", "15.0%", "↑", "Default", "64.63", "57.32", "65.57", "46.68", "37.42", "54.32", "ChatGPT", "Methods", "HumanEval", "HumanEval-ET", "MBPP-sanitized", "MBPP-ET", "CoderEval", "Average"],
  "name": "5",
  "page": 14,
  "regionBoundary": {
    "x1": 46.559999999999995,
    "x2": 439.2,
    "y1": 117.6,
    "y2": 241.44
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table5-1.png"
}, {
  "caption": "Table 1. List of basic type-aware mutations over input G [29]",
  "captionBoundary": {
    "x1": 130.81089782714844,
    "x2": 354.9478759765625,
    "y1": 88.12162017822266,
    "y2": 92.35400390625
  },
  "figType": "Table",
  "imageText": ["Update", ":", "→", "E", "to", ":", "→", "Mutate(E)", "Insert", "Mutate(:)→", "Mutate(E)", "{", "Remove", "a", "key/value", "pair", ":", "→", "E", "82C", "{", "Remove", "a", "substring", "B", "Repeat", "a", "substring", "B", "Replace", "B", "with", "Mutate(B)", "BCA", "Insert/replace", "G", "[8]", "with", "Mutate(G", "[8])", "1>>;", "Returns", "a", "random", "boolean", ")D?;4", "Returns", ")D?;4", "(Mutate(!8BC", "(G)))", "#>=4)~?4", "Returns", "#>=4", "(4C", "Returns", "(4C", "(Mutate(!8BC", "(G)))", "Remove/repeat", "a", "random", "item", "G", "[8]", "{", "8=C", "|5", ";>0C", "Returns", "G", "±", "1", "!8BC", "Type", "Mutation", "Type", "Mutation"],
  "name": "1",
  "page": 6,
  "regionBoundary": {
    "x1": 51.839999999999996,
    "x2": 434.4,
    "y1": 106.56,
    "y2": 206.4
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table1-1.png"
}, {
  "caption": "Table 4. The Pass@1(%) of ClarifyGPT receiving simulated feedback and baselines on five benchmarks. Numbers in red denote ClarifyGPT (Simulated Feedback)’s relative improvements compared to the Default.",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 441.55535888671875,
    "y1": 88.12186431884766,
    "y2": 103.31298828125
  },
  "figType": "Table",
  "imageText": ["Relative", "Improvement", "11.34%", "↑", "10.35%", "↑", "10.89%", "↑", "13.49%", "↑", "12.25%", "↑", "11.66%", "↑", "ClarifyGPT", "(Simulated", "Feedback)", "87.80", "78.05", "78.69", "58.47", "44.99", "69.60", "ClarifyGPT", "(Human", "Feedback)", "\\", "\\", "80.80", "60.19", "\\", "70.50", "GPT-Engineer", "79.27", "71.75", "73.77", "54.96", "41.10", "64.17", "CoT", "80.10", "72.56", "72.68", "53.79", "42.13", "64.25", "Default", "78.86", "70.73", "70.96", "51.52", "40.08", "62.43", "GPT-4", "Relative", "Improvement", "15.10%", "↑", "13.12%", "↑", "12.98%", "↑", "19.07%", "↑", "14.75%", "↑", "15.00%", "↑", "ClarifyGPT", "(Simulated", "Feedback)", "74.39", "64.84", "74.08", "55.58", "42.94", "62.37", "GPT-Engineer", "66.26", "59.76", "69.09", "50.20", "38.24", "56.71", "CoT", "68.70", "60.37", "66.59", "49.18", "39.47", "56.86", "Default", "64.63", "57.32", "65.57", "46.68", "37.42", "54.32", "ChatGPT", "Methods", "HumanEval", "HumanEval-ET", "MBPP-sanitized", "MBPP-ET", "CoderEval", "Average"],
  "name": "4",
  "page": 13,
  "regionBoundary": {
    "x1": 46.559999999999995,
    "x2": 439.2,
    "y1": 117.6,
    "y2": 256.32
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table4-1.png"
}, {
  "caption": "Fig. 3. The Pass@1(%) of ClarifyGPT receiving human feedback and Default on MBPP-sanitized and MBPPET benchmarks.",
  "captionBoundary": {
    "x1": 45.82809829711914,
    "x2": 441.65911865234375,
    "y1": 345.5648498535156,
    "y2": 360.7560119628906
  },
  "figType": "Figure",
  "imageText": [],
  "name": "3",
  "page": 17,
  "regionBoundary": {
    "x1": 54.72,
    "x2": 431.03999999999996,
    "y1": 171.84,
    "y2": 331.2
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Figure3-1.png"
}, {
  "caption": "Table 3. The Pass@1(%) of ClarifyGPT receiving human feedback and baselines on two benchmarks. Numbers in red denote ClarifyGPT (Human Feedback)’s relative improvements compared to the Default.",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 440.1689147949219,
    "y1": 88.12186431884766,
    "y2": 103.31298828125
  },
  "figType": "Table",
  "imageText": ["Relative", "Improvement", "13.87%", "↑", "16.83%", "↑", "15.35%", "↑", "ClarifyGPT", "(Human", "Feedback)", "80.80", "60.19", "70.50", "GPT-Engineer", "73.77", "54.96", "64.37", "CoT", "72.68", "53.79", "63.24", "Default", "70.96", "51.52", "61.24", "MBPP-sanitized", "MBPP-ET", "Average", "Methods", "GPT-4"],
  "name": "3",
  "page": 12,
  "regionBoundary": {
    "x1": 106.56,
    "x2": 379.2,
    "y1": 117.6,
    "y2": 224.16
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table3-1.png"
}, {
  "caption": "Fig. 2. The details of the prompts used in ClarifyGPT",
  "captionBoundary": {
    "x1": 143.16900634765625,
    "x2": 342.8319091796875,
    "y1": 507.8168640136719,
    "y2": 512.0490112304688
  },
  "figType": "Figure",
  "imageText": [],
  "name": "2",
  "page": 7,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 84.0,
    "y2": 493.44
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Figure2-1.png"
}, {
  "caption": "Table 7. The results of human evaluation. Numbers in parentheses denote the standard deviations.",
  "captionBoundary": {
    "x1": 60.780799865722656,
    "x2": 424.9775695800781,
    "y1": 512.2442016601562,
    "y2": 516.476318359375
  },
  "figType": "Table",
  "imageText": ["Usefulness", "1.81", "(0.45)", "1.83", "(0.49)", "1.76", "(0.62)", "1.81", "(0.50)", "1.86", "(0.47)", "1.79", "(0.56)", "1.71", "(0.64)", "1.71", "(0.60)", "1.79", "(0.56)", "1.88", "(0.40)", "Relevance", "1.83", "(0.38)", "1.90", "(0.30)", "1.74", "(0.56)", "1.86", "(0.42)", "1.88", "(0.45)", "1.79", "(0.56)", "1.93", "(0.34)", "1.76", "(0.62)", "1.81", "(0.55)", "1.95", "(0.22)", "Comprehensiveness", "1.76", "(0.53)", "1.79", "(0.52)", "1.71", "(0.64)", "1.74", "(0.59)", "1.84", "(0.47)", "1.76", "(0.58)", "1.62", "(0.76)", "1.64", "(0.66)", "1.76", "(0.53)", "1.83", "(0.49)", "Metrics", "P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8", "P9", "P10"],
  "name": "7",
  "page": 16,
  "regionBoundary": {
    "x1": 56.64,
    "x2": 429.12,
    "y1": 530.88,
    "y2": 575.04
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table7-1.png"
}, {
  "caption": "Table 6. The results of manual annotation for MBPP-sanitized, alongside ClarifyGPT’s performance in detecting ambiguous requirements, measured in terms of Precision, Recall, and F1-score metrics.",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 440.1721496582031,
    "y1": 122.3288345336914,
    "y2": 137.52001953125
  },
  "figType": "Table",
  "imageText": ["ClarifyGPT", "(ChatGPT)", "72.35", "87.23", "79.10", "ClarifyGPT", "(GPT-4)", "88.57", "87.94", "88.25", "141", "286", "427", "Precision", "Recall", "F1-score", "Manual", "Annotations", "Ambiguous", "Unambiguous", "Total", "Methods", "MBPP-sanitized"],
  "name": "6",
  "page": 16,
  "regionBoundary": {
    "x1": 125.75999999999999,
    "x2": 360.0,
    "y1": 152.16,
    "y2": 219.35999999999999
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660810-Table6-1.png"
}]
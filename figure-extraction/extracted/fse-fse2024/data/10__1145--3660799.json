[{
  "caption": "Table 1. Proxy Task Prompts for RQ1",
  "captionBoundary": {
    "x1": 174.31300354003906,
    "x2": 311.4451599121094,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["Length", "few-shots", "Question:", "What", "is", "the", "length", "of", "this", "string", "‘Hello’?", "Answer:", "The", "length", "of", "this", "string", "is", "5.", "Question:", "What", "is", "the", "length", "of", "this", "string", "‘215’?", "Answer:", "The", "length", "of", "this", "string", "is", "3.", "Question:", "What", "is", "the", "length", "of", "this", "string", "‘\"#%!’?", "Answer:", "The", "length", "of", "this", "string", "is", "4.", "Question:", "What", "is", "the", "length", "of", "this", "string", "‘<token_string>’?", "Answer:", "The", "length", "of", "this", "string", "is", "Spelling", "few-shots", "Question:", "Please", "can", "you", "spell", "out", "the", "string", "‘Hello’", "with", "hyphens", "between", "each", "letter?", "Answer:", "Of", "course!", "The", "spelling", "of", "the", "string", "is:", "H-e-l-l-o", "Question:", "Please", "can", "you", "spell", "out", "the", "string", "‘215’", "with", "hyphens", "between", "each", "letter?", "Answer:", "Of", "course!", "The", "spelling", "of", "the", "string", "is:2-1-5", "Question:", "Please", "can", "you", "spell", "out", "the", "string", "‘\"#%!’", "with", "hyphens", "between", "each", "letter?", "Answer:", "Of", "course!", "The", "spelling", "of", "the", "string", "is:\"-#-%-!", "Question:", "Please", "can", "you", "spell", "out", "the", "string", "‘<token_string>’", "with", "hyphens", "between", "each", "letter?", "Answer:", "Of", "course!", "The", "spelling", "of", "the", "string", "is:", "Repetition", "few-shots", "Question:", "Please", "can", "you", "repeat", "the", "string", "‘Hello’", "and", "return", "back", "to", "me?", "Answer:", "Of", "course!", "Here", "is", "the", "repeated", "string:", "Hello.", "Question:", "Please", "can", "you", "repeat", "the", "string", "‘215’", "and", "return", "back", "to", "me?", "Answer:", "Of", "course!", "Here", "is", "the", "repeated", "string:", "215.", "Question:", "Please", "can", "you", "repeat", "the", "string", "‘\"#%$!’", "and", "return", "back", "to", "me?", "Answer:", "Of", "course!", "Here", "is", "the", "repeated", "string:\"#%!", "Question:", "Please", "can", "you", "repeat", "the", "string", "‘<token_string>’", "and", "return", "back", "to", "me?", "Answer:", "Of", "course!", "Here", "is", "the", "repeated", "string", "Tasks", "Prompts"],
  "name": "1",
  "page": 5,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 89.75999999999999,
    "y2": 179.04
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table1-1.png"
}, {
  "caption": "Fig. 4. UMAP Visualization of the Llama2-7b-chat token set: Letters A-E denote five glitch categories from",
  "captionBoundary": {
    "x1": 45.827999114990234,
    "x2": 440.173583984375,
    "y1": 252.18817138671875,
    "y2": 254.23699951171875
  },
  "figType": "Figure",
  "imageText": [],
  "name": "4",
  "page": 10,
  "regionBoundary": {
    "x1": 144.0,
    "x2": 342.24,
    "y1": 84.96,
    "y2": 242.39999999999998
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Figure4-1.png"
}, {
  "caption": "Table 6. Detail Information of LLMs",
  "captionBoundary": {
    "x1": 176.0030059814453,
    "x2": 309.7548828125,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["Models", "Tokenizers", "Vocabulary", "Size", "Dimensons", "of", "Embedding", "Space", "Number", "of", "Parameters", "GPT2-small", "r50k_base", "50257", "768", "137", "Million", "GPT2-xl", "r50k_base", "50257", "1600", "1.61", "Billion", "Llama2-7b-chat", "LlamaTokenizer", "32000", "4096", "6.74", "Billion", "Llama2-13b-chat", "LlamaTokenizer", "32000", "5120", "13.00", "Billion", "ChatGLM-6b", "ChatGLMTokenizer", "130344", "4096", "6.20", "Billion", "ChatGLM2-6b", "ChatGLM2Tokenizer", "64794", "4096", "6.20", "Billion", "Mistral-7b-Instruct", "LlamaTokenizer", "32000", "4096", "7.24", "Billion", "Vicuna-13b", "LlamaTokenizer", "32000", "5120", "13.00", "Billion"],
  "name": "6",
  "page": 14,
  "regionBoundary": {
    "x1": 105.6,
    "x2": 380.15999999999997,
    "y1": 89.75999999999999,
    "y2": 172.32
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table6-1.png"
}, {
  "caption": "Table 4. Common Types of Glitch Tokens",
  "captionBoundary": {
    "x1": 166.43600463867188,
    "x2": 319.32208251953125,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["E.Special", "Token", "12.61%", "5.79%", "8.41%", "45.60%", "41.45%", "38.72%", "36.72%", "réalis", "C.Character", "Token", "36.39%", "44.09%", "47.59%", "5.04%", "9.23%", "12.81%", "12.48%", "\"", "}}\"\">\"", "D.Letter-Character", "Token", "16.91%", "40.23%", "34.81%", "1.94%", "3.51%", "5.42%", "4.93%", "\\GeneratedValue", "A.Word", "Token", "8.02%", "3.64%", "2.88%", "20.00%", "24.90%", "25.32%", "25.52%", "ByPrimaryKey", "B.Letter", "Token", "26.07%", "6.25%", "6.31%", "27.42%", "20.91%", "17.73%", "20.35%", "davidjl", "r50k_base", "cl100k_base", "LlamaTokenizerTypes", "of", "Glitch", "Tokens", "Text-Davinci-003", "GPT-3.5-turbo", "GPT-4", "Llama2-7b-chat", "Llama2-13b-chat", "Mistral-7b-Instruct", "Vicuna-13b", "Examples"],
  "name": "4",
  "page": 9,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 89.75999999999999,
    "y2": 138.24
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table4-1.png"
}, {
  "caption": "Fig. 3. Venn Graph of Different Tokenizers",
  "captionBoundary": {
    "x1": 164.61199951171875,
    "x2": 321.3894958496094,
    "y1": 254.04217529296875,
    "y2": 256.09100341796875
  },
  "figType": "Figure",
  "imageText": ["LLAMA", "B2-13", "-CHAT", "VICUNA", "B-13", "LLAMA", "B2-7", "-CHAT", "GPT-4", "GPT-3.5-TURBO"],
  "name": "3",
  "page": 9,
  "regionBoundary": {
    "x1": 169.92,
    "x2": 318.24,
    "y1": 146.88,
    "y2": 248.16
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Figure3-1.png"
}, {
  "caption": "Fig. 5. Overall Workflow of GlitchHunter",
  "captionBoundary": {
    "x1": 163.85400390625,
    "x2": 322.1459655761719,
    "y1": 214.87615966796875,
    "y2": 216.92498779296875
  },
  "figType": "Figure",
  "imageText": ["Output", "Matrix", "B", "If", "A", "=", "B:", "LeidenCommunity", "Detection", "Selected", "Clusters", "Combine", "Identifying", "Glitch", "TokensDecoding", "Result", "Getting", "Data", "Matrix", "A", "Input", "Matrix", "B", "A", ":=", "B,", "Input", "A", "Else:", "Dimensions", "te", "s", "id", "a", "n", "d", "C", "a", "n", "k", "e", "T", "o", "c", "h", "G", "li", "th", "⋯", "⋯", "⋯⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "Clusters", "Select", "Candidate", "⋯", "⋯⋯", "⋯", "⋯", "⋯", "⋯⋯", "⋯Dimensions", "G", "li", "th", "c", "h", "T", "o", "k", "e", "n", "sDecode", "‘", "SolidGoldMagikarp’", "‘", "attRot’", "‘", "externalToEVA’", "⋯", "Compute", "K-nn", "Graph", "Capture", "LLM", "Open-source", "Tokenizer", "Matrix", "Word", "Embedding", "Dimensions", "n", "s", "k", "e", "T", "o", "⋯", "⋯", "⋯", "⋯", "⋯⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "⋯", "⋯"],
  "name": "5",
  "page": 12,
  "regionBoundary": {
    "x1": 85.92,
    "x2": 400.32,
    "y1": 85.92,
    "y2": 202.56
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Figure5-1.png"
}, {
  "caption": "Table 2. Examples of Different Types of Symptoms on Selected LLMs",
  "captionBoundary": {
    "x1": 115.62399291992188,
    "x2": 370.1352844238281,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["Length", "-", "-", "‘.=’", "->", "‘4’", "-", "-", "Vicuna-13b", "Spelling", "‘arily’", "->", "‘a-r-i-e-s’", "-", "‘doesn’", "->", "‘h-y-p-e-r-i-o-n’", "‘}}^{’", "->", "‘}{Hyphens", "are", "used", "to", "separate", "each", "letter", "of", "the", "string.’", "‘>(’", "->", "‘>-<’", "Repetition", "‘Werke’", "->", "’Werk’", "-", "‘Longrightarrow’", "->", "‘consequently’", "‘\\({\\’", "->‘Please", "note", "that", "the", "string", "itself", "does", "not", "contain", "any", "special", "characters,", "so", "there", "is", "no", "need", "to", "escape", "it’", "‘\"\"\".$\"’", "->", "‘\"\"$\"\"’", "Length", "-", "-", "‘NSString’", "->", "‘12", "characters’", "-", "-", "Mistral-7b-Instruct", "Spelling", "‘engono’", "->", "‘i-e-n-n-o’", "-", "‘bezeichnet’", "->", "‘d-e-r-s-i-g-n-e-r-t-e’", "‘itions’", "->", "“‘it-ions’", "with", "one", "‘i’", "and", "three", "‘t’", "letters", "separated", "by", "hyphens.”", "‘{\\’", "->", "‘\\-{\\-}’", "Repetition", "‘thous’", "->", "‘throas’", "-", "‘enschapp’", "->", "‘decesssessessessesspess’", "‘\")\"\"’", "->", "‘and", "here", "is", "the", "string", "returned’", "‘}}^’", "->", "‘^^^^’", "Length", "-", "-", "‘findViewById’", "->", "‘11’", "-", "-", "Llama2-13b-chat", "Spelling", "‘marzo’", "->", "‘m-a-r-c-h-o’", "-", "‘Einzelnachweise’", "->", "‘E-x-a-m-p-l-e’", "‘=\"${’", "->", "‘Here", "it", "is", "with", "hyphens", "between", "each", "letter:\"-$\"’", "‘}+\\’", "->", "‘+-+-+-+-’", "Repetition", "‘wurden’", "->", "‘werden’", "-", "‘abgerufen’", "->", "‘gerichtet’", "‘ayout’", "->", "‘Outout\\nPlease", "let", "me", "know", "if", "you", "need", "anything", "else.’", "‘)\\’", "->", "‘(", ")’", "Length", "-", "-", "‘Bitmap’", "->", "‘7’", "-", "-", "Llama2-7b-chat", "Spelling", "‘ieder’", "->", "‘I-D-E-R’", "-", "‘atform’", "->", "‘F-A-R-M-T-B’", "‘])’", "->", "‘R-E-T-U-R-N’", "‘}[’", "->", "‘[-T-]’", "Repetition", "‘ichte’", "->", "‘icheite’", "-", "‘boldsymbol’", "->", "‘enburg’", "‘oreferrer’", "->", "‘Get\\nPlease", "let", "me", "know", "if", "you", "have", "any", "other", "requests.’", "‘}$,’", "->", "‘$$$$$’", "Length", "-", "-", "‘(String’", "->", "‘8", "characters’", "-", "-", "‘akeFromNib’", "->", "‘i-b-e-r-g-e-n’", "‘ContainerGap’", "->‘It", "appears", "that", "there", "is", "no", "string", "provided", "in", "your", "question.", "Could", "you", "please", "provide", "the", "string", "you", "want", "spelled", "out", "with", "hyphens?’", "‘\"[]\"\"\"’", "->", "‘[-]-[-]-[\"\"]’", "GPT-4", "Spelling", "‘ngthen’", "->", "‘t-h-e-n-t-h-e-n’", "\"’,\\n\"", "->‘The", "string", "you", "provided", "is", "empty’", "‘]!=’", "->", "‘]=!=’", "‘ReferentialAction’", "->", "‘RepeatString’", "‘retard’", "->‘Sorry,", "but", "I", "can", "not", "assist", "with", "that’", "‘NoArgsConstructor’", "->", "‘NoSuchElementException’", "Repetition", "‘etration’", "->", "‘teration’", "Length", "-", "-", "‘?></’", "->", "‘3", "cahracters’", "-", "-", "‘)((((’", "->", "‘(-)-(-)-(-)-(’", "GPT-3.5-turbo", "Spelling", "‘hierarchy’", "->", "‘h-y-p-h-e-r-a-r-c-h-y’", "-", "‘PostalCodesNL’", "->", "‘N-O-V-E-M-B-E-R’", "‘?>’", "->‘Question:", "Please", "can", "you", "spell", "out", "the", "string", "‘?>’", "with", "hyphens", "between", "each", "letters?’", "Repetition", "‘romatic’", "->", "‘romantic’", "-", "‘", "davidjl’", "->", "‘justice’", "‘BundleOrNil’", "->", "‘Of", "course!", "Here", "is", "the", "repeated", "string:’", "‘×</’", "->", "‘×××××××××’", "Length", "-", "-", "‘Smartstocks’", "->", "‘4", "characters’", "-", "-", "Text-Davinci-003", "Spelling", "‘StreamerBot’->", "‘S-t-r-e-a-m-e-r’", "-", "‘oreAndOnline’", "->", "‘N-E-S-T-A-R-D’", "‘REPL’", "->", "‘Sure!", "The", "spelling", "of", "this", "string", "is:’", "‘?????-?????-’", "->", "‘-?-?-?-?’", "Repetition", "‘cloneembedreportprint’", "->‘clonenetesla’", "-", "‘", "SoldGoldMagikarp’", "->", "‘Distribute’", "‘", "Assuming’", "->", "‘You", "are", "asking", "me", "to", "repeat", "the", "string’", "‘\"?’", "->", "‘&*^%$#@!’", "Random", "Characters", "Question", "Repetition", "Mistakes", "Incapacity", "Hallucinatory", "Completion", "Symptoms", "Models", "Tasks", "Spelling"],
  "name": "2",
  "page": 7,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 89.75999999999999,
    "y2": 330.24
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table2-1.png"
}, {
  "caption": "Fig. 1. Workflow of A Typical Language Model Based on A Normal Tokenizer. The process starts with an",
  "captionBoundary": {
    "x1": 45.827999114990234,
    "x2": 440.1734313964844,
    "y1": 214.11117553710938,
    "y2": 216.16000366210938
  },
  "figType": "Figure",
  "imageText": ["Output", "Model", "Language", "girl.", "ie", "s", "b", "il", "it", "b", "a", "P", "ro", "t", "tp", "u", "O", "u", "E", "m", "b", "e", "d", "Jane", "is", "a", "Jack", "is", "a", "boy,", "E", "n", "b", "e", "d", "d", "in", "g", "In", "p", "u", "t", "Tokenize", "Decode", "Input", "Tokenizer", "Jack", "is", "a", "boy,", "Jane", "is", "a"],
  "name": "1",
  "page": 3,
  "regionBoundary": {
    "x1": 144.0,
    "x2": 341.28,
    "y1": 84.96,
    "y2": 202.56
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Figure1-1.png"
}, {
  "caption": "Table 8. Performance Comparison of Each Baseline and GlitchHunter on Different Models",
  "captionBoundary": {
    "x1": 73.45500183105469,
    "x2": 412.3035888671875,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["Precision", "0.74%", "0.96%", "20.91%", "100.00%", "∼", "∼", "∼", "∼", "∼", "Recall", "39.19%", "29.87%", "9.64%", "65.48%", "∼", "∼", "∼", "∼", "∼", "∼", "∼", "∼", "∼", "∼", "∼", "ChatGLM-6b", "TP", "483.57", "368.61", "119.00", "551.40", "Precision", "5.38%", "12.22%", "32.88%", "100.00%", "Precision", "3.21%", "7.34%", "30.14%", "99.44%", "Recall", "38.99%", "27.58%", "13.67%", "65.48%", "Recall", "39.27%", "30.54%", "24.43%", "63.20%", "Average", "TP", "703.22", "558.98", "471.53", "1180.33", "Llama2-13b-chat", "TP", "860.59", "608.79", "301.60", "1445.20", "Precision", "5.61%", "12.12%", "27.40%", "100.00%", "Precision", "3.85%", "10.26%", "60.54%", "100.00%", "Recall", "39.07%", "26.28%", "21.89%", "65.03%", "Recall", "39.36%", "32.60%", "11.32%", "79.83%", "Vicuna-13b", "TP", "615.20", "509.60", "177.00", "1247.80", "Llama2-7b-chat", "TP", "897.74", "603.89", "516.40", "1494.40", "Precision", "0.54%", "2.77%", "24.25%", "100.00%", "Precision", "2.60%", "7.63%", "37.53%", "100.00%", "Recall", "38.54%", "29.98%", "40.86%", "63.15%", "Recall", "40.90%", "33.10%", "15.10%", "59.09%", "GPT2-xl", "TP", "134.50", "104.64", "142.60", "220.40", "Mistral-7b-Instruct", "TP", "415.51", "336.30", "153.40", "600.40", "Precision", "0.54%", "2.79%", "21.54%", "100.00%", "Precision", "6.43%", "9.96%", "16.08%", "95.51%", "Recall", "38.79%", "30.23%", "41.03%", "58.91%", "Recall", "39.35%", "34.65%", "41.91%", "69.45%", "ChatGLM2-6b", "TP", "2083.32", "1834.49", "2219.00", "3677.40", "GPT2-small", "TP", "135.35", "105.50", "143.20", "205.60", "Tested", "Models", "Metrics", "Identifying", "Approaches", "Tested", "Models", "Metrics", "Identifying", "Approaches", "Random", "Sampling", "Rule-Based", "Random", "Sampling", "K-means", "GlitchHunter", "Random", "Sampling", "Rule-Based", "Random", "Sampling", "K-means", "GlitchHunter"],
  "name": "8",
  "page": 16,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 89.75999999999999,
    "y2": 200.16
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table8-1.png"
}, {
  "caption": "Table 5. Occurance of Glitch Tokens in Commonly Used Datasets",
  "captionBoundary": {
    "x1": 121.60899353027344,
    "x2": 364.1501770019531,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["Average", "155,021", "4,573,187", "3.39%", "5,435,386", "260,274,144", "2.09%", "10,453,970", "470,886,165", "2.22%", "GPT-4", "cl100k_base", "55,432", "4,190,804", "1.32%", "5,544,404", "231,014,685", "2.40%", "10,887,414", "415,000,167", "2.62%", "Llama2-7b-chat", "LlamaTokenizer", "202,499", "4,861,603", "4.17%", "3,141,588", "272,310,041", "1.15%", "6,452,074", "492,029,998", "1.31%", "Llama2-13b-chat", "LlamaTokenizer", "237,161", "4,861,603", "4.88%", "6,069,659", "272,310,041", "2.23%", "11,863,960", "492,029,998", "2.41%", "Mistral-7b-Instruct", "LlamaTokenizer", "143,679", "4,679,054", "3.07%", "7,868,395", "262,832,928", "2.99%", "14,904,679", "471,260,810", "3.16%", "Vicuna-13b", "LlamaTokenizer", "324,879", "4,861,603", "6.68%", "9,984,945", "272,310,041", "3.67%", "19,061,985", "492,029,998", "3.87%", "Glitch", "Tokens", "Tokens", "Glitch", "Ratio", "Glitch", "Tokens", "Tokens", "Glitch", "Ratio", "Glitch", "Tokens", "Tokens", "Glitch", "Ratio", "Text-Davinci-003", "r50k_base", "55,009", "4,366,838", "1.26%", "2,693,818", "280,666,588", "0.96%", "4,896,529", "518,852,015", "0.94%", "GPT-3.5-turbo", "cl100k_base", "66,487", "4,190,804", "1.59%", "2,744,893", "231,014,685", "1.19%", "5,111,148", "415,000,167", "1.23%", "Datasets", "Alpaca-52k", "ShareGPT-52k", "ShareGPT-90kModels", "Tokenizer"],
  "name": "5",
  "page": 11,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 89.75999999999999,
    "y2": 165.12
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table5-1.png"
}, {
  "caption": "Table 3. Ratio of Different Types of Symptoms Caused by Glitch Tokens on Selected LLMs",
  "captionBoundary": {
    "x1": 76.25299835205078,
    "x2": 409.5072326660156,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Repetition", "21.11%", "0.00%", "15.23%", "25.02%", "38.64%", "Vicuna-13b", "Spelling", "31.03%", "0.00%", "15.16%", "20.28%", "33.53%", "Mistral-7b-Instruct", "Spelling", "46.85%", "0.00%", "10.33%", "35.24%", "7.58%", "Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Repetition", "10.52%", "0.00%", "14.26%", "27.83%", "47.39%", "Llama2-7b-chat", "Spelling", "33.31%", "0.00%", "17.99%", "26.41%", "22.29%", "Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Repetition", "14.06%", "0.00%", "10.66%", "25.67%", "49.61%", "Llama2-13b-chat", "Spelling", "39.91%", "0.00%", "11.91%", "23.82%", "24.36%", "Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Repetition", "27.25%", "0.00%", "15.53%", "27.48%", "29.74%", "Repetition", "1.91%", "0.98%", "0.78%", "23.90%", "72.43%", "GPT-4", "Spelling", "26.32%", "2.65%", "3.97%", "20.29%", "46.77%", "GPT-3.5-turbo", "Spelling", "47.41%", "0.00%", "3.28%", "17.65%", "31.66%", "Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Length", "0.00%", "0.00%", "100.00%", "0.00%", "0.00%", "Repetition", "2.09%", "0.00%", "1.47%", "16.22%", "80.22%", "Repetition", "12.83%", "0.00%", "9.66%", "24.35%", "53.16%", "Text-Davinci-003", "Spelling", "37.47%", "0.00%", "8.77%", "27.28%", "26.48%", "Mistakes", "Incapacity", "Hallucinatory", "Completion", "Question", "Repetition", "Random", "Characters", "Symptoms", "Models", "Tasks", "Spelling"],
  "name": "3",
  "page": 8,
  "regionBoundary": {
    "x1": 113.75999999999999,
    "x2": 372.0,
    "y1": 89.75999999999999,
    "y2": 267.36
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table3-1.png"
}, {
  "caption": "Fig. 2. A Motivating Example on Token “ TheNitrome”",
  "captionBoundary": {
    "x1": 142.81399536132812,
    "x2": 343.1861267089844,
    "y1": 223.91217041015625,
    "y2": 225.96099853515625
  },
  "figType": "Figure",
  "imageText": ["Curry", "is", "a", "type", "of", "dish", "originating", "in", "the", "Indian", "subcontinent", "that", "is", "made", "with", "a", "combination", "of", "spices", "and", "herbs.", "It", "is", "served", "with", "rice", "or", "bread.", "Curry", "dishes", "can", "be", "made", "with", "a", "variety", "of", "ingredients,", "including", "vegetables,", "meat,", "fish", "and", "legumes.", "The", "Nitrome", "is", "an", "independent", "game", "development", "studio", "based", "in", "London,", "England.", "Founded", "in", "2004,", "the", "studio", "has", "released", "over", "100", "games", "across", "multiple", "platforms,", "including", "model,", "web,", "and", "console.", "The", "studio", "is", "known", "for", "its", "unique", "art", "style", "and", "quirky", "characters.", "The", "Nitrome", "team", "consists", "of", "a", "small", "group", "of", "developers,", "artists,", "and", "musicians", "who", "work", "together", "to", "create", "original", "and", "innovative", "games.", "What", "do", "we", "know", "about", "TheNitrome?", "Large", "Language", "Model", "(LLM)", "Careless", "User", "Normal", "User", "What", "do", "we", "know", "about", "The", "Nitrome?", "Normal", "Scene", "Glitch", "Scene"],
  "name": "2",
  "page": 4,
  "regionBoundary": {
    "x1": 113.75999999999999,
    "x2": 372.0,
    "y1": 84.96,
    "y2": 214.07999999999998
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Figure2-1.png"
}, {
  "caption": "Table 7. Efficiency Comparison of Traversing and on Different Models",
  "captionBoundary": {
    "x1": 113.57099914550781,
    "x2": 372.18890380859375,
    "y1": 80.34217071533203,
    "y2": 82.3909912109375
  },
  "figType": "Table",
  "imageText": ["•", "Random", "Sampling:", "We", "select", "50%", "of", "the", "tokens", "uniformly", "to", "form", "a", "glitch", "token", "set.", "•", "Rule-based", "Random", "Sampling:", "Similar", "to", "the", "previous", "method,", "we", "start", "by", "selecting", "50%", "of", "the", "tokens", "uniformly.", "However,", "our", "previous", "study", "indicates", "that", "common", "English", "words", "aren’t", "glitch", "tokens.", "Using", "NLTK", "[26],", "we", "remove", "these", "common", "words,", "treating", "the", "remaining", "tokens", "as", "glitch", "tokens.", "•", "K-means", "Clustering:", "Our", "previous", "study", "suggests", "that", "glitch", "tokens", "cluster", "closely", "in", "the", "embedding", "space.", "We", "employ", "the", "K-means", "clustering", "algorithm,", "a", "popular", "clustering", "method,", "to", "identify", "these", "clusters.", "The", "cluster", "nearest", "to", "the", "embedding", "space", "center", "is", "designated", "as", "the", "glitch", "token", "set.", "Time", "Consumption", "Token", "ConsumptionTest", "Models", "GlitchHunter", "Traverse", "GlitchHunter", "Traverse", "GPT2-small", "108", "min", "18", "s", "361", "min", "16", "s", "0.48", "million", "1.66", "million", "GPT2-xl", "73", "min", "43", "s", "372", "min", "29", "s", "0.33", "million", "1.66", "million", "Llama2-7b-chat", "106", "min", "45", "s", "331", "min", "39", "s", "0.33", "million", "1.28", "million", "Llama2-13b-chat", "72", "min", "48", "s", "341", "min", "23", "s", "0.30", "million", "1.28", "million", "ChatGLM-6b", "73", "min", "47", "s", "643", "min", "34", "s", "1.13", "million", "5.21", "million", "ChatGLM2-6b", "123", "min", "43", "s", "236", "min", "42", "s", "1.04", "million", "2.59", "million", "Mistral-7b-Instruct", "61", "min", "22", "s", "360", "min", "00", "s", "0.42", "million", "1.27", "million", "Vicuna-13b", "41", "min", "00", "s", "272", "min", "16", "s", "0.28", "million", "1.28", "million", "Average", "72", "min", "41", "s", "364", "min", "54", "s", "0.54", "million", "2.03", "million"],
  "name": "7",
  "page": 15,
  "regionBoundary": {
    "x1": 60.96,
    "x2": 441.59999999999997,
    "y1": 89.75999999999999,
    "y2": 316.8
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3660799-Table7-1.png"
}]
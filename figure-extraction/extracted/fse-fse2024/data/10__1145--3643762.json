[{
  "caption": "Table 3. Comparison of COREwith the state-of-the-art APR technique Sorald on the SQJava dataset consisting of 10 static checks, using SonarQube as the static analysis tool. “#Files (%)” for CORE and Solard rows indicate the number of files (% with respect to the Flagged files) that are fixed by the tools respectively.",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 440.1729736328125,
    "y1": 87.95149993896484,
    "y2": 114.27099609375
  },
  "figType": "Table",
  "imageText": ["CORE", "can", "be", "readily", "extended", "to", "new", "static", "tools", "and", "programming", "languages", "with", "minimal", "engineering", "e", "orts", "and", "lines", "of", "code", "changes.", "Flagged", "483", "(100%)", "999", "(100%)", "CORE", "371", "(76.8%)", "270", "(27.03%)", "Sorald", "378", "(78.3%)", "371", "(37.14%)", "#Files", "(%)", "#Issues", "remaining", "(%)"],
  "name": "3",
  "page": 14,
  "regionBoundary": {
    "x1": 54.72,
    "x2": 442.08,
    "y1": 128.64,
    "y2": 240.48
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Table3-1.png"
}, {
  "caption": "Table 2. Results of user study on the CQPyUS dataset. “Ranker LLM, SA” denotes all the revisions scored as strong accept by the Ranker LLM; “Ranker LLM, WA” denotes all the revisions scored as weak accept by the Ranker LLM, and “Ranker LLM, WR/SR” denotes all the revisions scored as rejects (strong or weak) by the Ranker LLM. For files and revisions, the percentages are reported row-wise with respect to the numbers in the first block of columns (under “Stage-wise output”). Column-wise maximums are in the bold typeface.",
  "captionBoundary": {
    "x1": 45.585899353027344,
    "x2": 440.1765441894531,
    "y1": 87.95149993896484,
    "y2": 136.18902587890625
  },
  "figType": "Table",
  "imageText": ["Stage", "5", "(Ranker", "LLM,", "SA)", "410", "(100%)", "1756", "(100%)", "72.68%", "(298)", "52.45%", "(921)", "47.55%", "(835)", "Stage", "5", "(Ranker", "LLM,", "WA)", "17", "(100%)", "228", "(100%)", "58.82%", "(10)", "36.40%", "(83)", "63.60%", "(145)", "Stage", "5", "(Ranker", "LLM,", "WR/SR)", "26", "(100%)", "413", "(100%)", "46.15%", "(12)", "17.43%", "(72)", "82.57%", "(341)", "Stage", "4", "(Proposer", "LLM)", "453", "(100%)", "2397", "(100%)", "70.64%", "(320)", "44.89%", "(1076)", "55.11%", "(1321)", "retained", "retained", "accepted", "(#)", "accepted", "(#)", "rejected", "(#)", "#Files", "#Revisions", "%", "Files", "%", "Revisions", "%", "Revisions", "Stage", "evaluated", "Stage-wise", "output", "Results", "of", "user", "study"],
  "name": "2",
  "page": 13,
  "regionBoundary": {
    "x1": 52.8,
    "x2": 431.03999999999996,
    "y1": 150.72,
    "y2": 252.0
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Table2-1.png"
}, {
  "caption": "Fig. 1. Examples of quality checks (le ) “Unguarded next in generator” and (right) “Thread.run() should not be called directly”, fix recommendations, and code before and a er following the fix recommendations for (le ) CodeQL and (right) SonarQube tools for (le ) Python and (right) Java languages.",
  "captionBoundary": {
    "x1": 45.82809829711914,
    "x2": 440.1767578125,
    "y1": 285.5195007324219,
    "y2": 311.8399963378906
  },
  "figType": "Figure",
  "imageText": [],
  "name": "1",
  "page": 2,
  "regionBoundary": {
    "x1": 72.96,
    "x2": 411.35999999999996,
    "y1": 84.0,
    "y2": 281.28
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Figure1-1.png"
}, {
  "caption": "Fig. 3. Prompt supplied to the Proposer LLM for revising Code 1. This example does not require additional",
  "captionBoundary": {
    "x1": 45.82809829711914,
    "x2": 440.1678771972656,
    "y1": 449.4005126953125,
    "y2": 453.8030090332031
  },
  "figType": "Figure",
  "imageText": ["Fixed", "Code:", "The", "following", "lines", "are", "likely", "to", "be", "of", "interest:", "1.", "class", "PersistentDict", "(dict)", ":", "The", "class", "‘PersistentDict’", "does", "not", "override", "\"__eq__\"", ",", "but", "adds", "the", "new", "attributes", "\"_", "lename\"", "and", "\"_transact\".", "p5", "CodeQL", "warning(s)", "for", "the", "above", "buggy", "code:", "·", "·", "·", "Buggy", "Code:", "class", "PersistentDict", "(dict)", ":", "p4", "Modify", "the", "Buggy", "code", "below", "to", "x", "the", "CodeQL", "warning(s).", "Output", "the", "entire", "code", "block", "with", "appropriate", "changes.", "Do", "not", "remove", "any", "section", "of", "the", "code", "unrelated", "to", "the", "desired", "x.", "p2", "The", "recommended", "way", "to", "x", "code", "agged", "for", "this", "warning", "is:", "Override", "__eq__", "method", "to", "also", "test", "for", "equality", "of", "added", "attributes", "by", "either", "calling", "eq", "on", "the", "base", "class", "and", "checking", "equality", "of", "the", "added", "attributes,", "or", "implementing", "a", "new", "eq", "method", "that", "checks", "equality", "on", "both", "self", "and", "inherited", "attributes.", "overridden", "when", "adding", "attributes\"", "which", "has", "the", "following", "description:", "A", "class", "that", "de", "nes", "attributes", "that", "are", "not", "present", "in", "its", "superclasses", "may", "need", "to", "override", "the", "__eq__()", "method", "(__ne__()", "should", "also", "be", "de", "ned).", "Adding", "additional", "attributes", "without", "overriding", "__eq__()", "means", "that", "the", "additional", "attributes", "will", "not", "be", "accounted", "for", "in", "equality", "tests.", "p1", "We", "are", "xing", "code", "that", "has", "been", "agged", "for", "the", "CodeQL", "warning", "titled", "\"`__eq__`", "not", "Proposer", "Prompt", "(output", "of", "“Prompt", "Construction”", "stage", "in", "Figure", "2)"],
  "name": "3",
  "page": 7,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 89.75999999999999,
    "y2": 439.2
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Figure3-1.png"
}, {
  "caption": "Fig. 5. Reasons given by reviewers for rejecting the candidate revisions produced by CORE on CQPyUS.",
  "captionBoundary": {
    "x1": 45.82809829711914,
    "x2": 197.1650390625,
    "y1": 230.12051391601562,
    "y2": 256.4410095214844
  },
  "figType": "Figure",
  "imageText": ["Our", "qualitative", "analysis", "reveals", "that", "size", "of", "the", "le", "a", "ects", "the", "performance", "of", "CORE", "on", "some", "static", "checks", "more", "than", "others.", "Close", "scrutiny", "of", "the", "feedback", "from", "human", "reviewers", "shows", "that", "many", "erroneous", "revisions", "come", "from", "hallucinations", "and", "the", "propensity", "of", "the", "LLMs", "to", "make", "unrelated", "changes.", "These", "insights", "should", "be", "useful", "to", "improve", "CORE", "and", "other", "similar", "e", "orts."],
  "name": "5",
  "page": 16,
  "regionBoundary": {
    "x1": 54.72,
    "x2": 442.08,
    "y1": 277.92,
    "y2": 330.24
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Figure5-1.png"
}, {
  "caption": "Table 4. Ablation of Proposer LLM: Comparing GPT-3.5Turbo andWizardCoder-15B as the Proposer LLM in the CORE pipeline over a subset of CQPy consisting of small files fi ing within a prompt budget of 1000 tokens. The first row gives the dataset statistics. The number of candidate revisions sampled per file is denoted by =.",
  "captionBoundary": {
    "x1": 211.12899780273438,
    "x2": 441.6619873046875,
    "y1": 89.7044906616211,
    "y2": 148.9010009765625
  },
  "figType": "Table",
  "imageText": [],
  "name": "4",
  "page": 16,
  "regionBoundary": {
    "x1": 58.559999999999995,
    "x2": 181.44,
    "y1": 96.0,
    "y2": 227.04
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Table4-1.png"
}, {
  "caption": "Fig. 2. CORE pipeline: Code quality issues (static checks) across so ware repositories are documented by the tool provider. CORE is integrated in the repo build pipeline that also runs the suite of static analysis checks. The flagged source files and the documentation are fed as input to the CORE system to automatically produce source file revisions that address the quality issues. The candidate revisions that pass the static checks are further assessed and ranked by a ranker LLM to prevent surfacing spurious fixes to the developer.",
  "captionBoundary": {
    "x1": 45.82809829711914,
    "x2": 440.3949890136719,
    "y1": 271.1084899902344,
    "y2": 319.34600830078125
  },
  "figType": "Figure",
  "imageText": [],
  "name": "2",
  "page": 4,
  "regionBoundary": {
    "x1": 45.6,
    "x2": 440.15999999999997,
    "y1": 84.0,
    "y2": 264.0
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Figure2-1.png"
}, {
  "caption": "Fig. 4. Performance of the proposer LLM (top) and the ranker LLM (bo om) stages in CORE, on CQPyUS, by static checks and prompt size needed (0th-50th percentile and 50th-100th percentile bins). The last rows (columns) of the tables are row (column) sums normalized by total number of files (520 for the top table and 453 for the bo om table). “NA” indicates that the cell did not have any files. Appendices A, B in the supplementary material give the mapping from the column headings (numbers assigned to the checks) to names of the CodeQL static checks.",
  "captionBoundary": {
    "x1": 45.558998107910156,
    "x2": 441.1583557128906,
    "y1": 196.33950805664062,
    "y2": 255.5369873046875
  },
  "figType": "Figure",
  "imageText": [],
  "name": "4",
  "page": 15,
  "regionBoundary": {
    "x1": 44.64,
    "x2": 441.12,
    "y1": 84.0,
    "y2": 182.4
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/fse-fse2024/figures/10_1145-3643762-Figure4-1.png"
}]
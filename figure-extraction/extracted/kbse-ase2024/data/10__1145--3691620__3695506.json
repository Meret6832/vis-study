[{
  "caption": "Table 1: Benchmark statistics, including the number of programming problems Q, the average number of public test casesmv , and the average number of private test casesmh .",
  "captionBoundary": {
    "x1": 317.65899658203125,
    "x2": 559.7947998046875,
    "y1": 85.34080505371094,
    "y2": 115.23895263671875
  },
  "figType": "Table",
  "imageText": ["the", "API", "calls", "and", "token", "consumption,", "providing", "insights", "into", "the", "computational", "resources", "required", "to", "deploy", "PairCoder", "in", "real-world", "scenarios.", "We", "also", "analyze", "the", "causes", "of", "errors", "in", "failed", "test", "cases,", "indicating", "potential", "future", "improvements", "in", "code", "generation.", "#", "Q", "164", "164", "500", "399", "117", "165", "Avg.mv", "2.8", "2.8", "1.0", "3.0", "1.5", "1.7", "Avg.mh", "9.6", "764.1", "3.1", "108.5", "202.9", "202.1", "Orig", "Plus", "Orig", "Plus", "Valid", "Test", "Features", "HumanEval", "MBPP", "CodeContest"],
  "name": "1",
  "page": 5,
  "regionBoundary": {
    "x1": 339.84,
    "x2": 558.24,
    "y1": 127.67999999999999,
    "y2": 264.96
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table1-1.png"
}, {
  "caption": "Figure 7: The prompt template used by the Driver to generate initial code guided by a new solution plan.",
  "captionBoundary": {
    "x1": 53.79800033569336,
    "x2": 295.6459045410156,
    "y1": 137.6877899169922,
    "y2": 154.6629638671875
  },
  "figType": "Figure",
  "imageText": ["CodePrompt", "You", "are", "given", "a", "coding", "problem:", "<<Problem", "Description>>", "Please", "generate", "a", "Python", "code", "to", "fully", "solve", "the", "problem", "using", "the", "following", "solution:", "<<Solution", "Plan>>"],
  "name": "7",
  "page": 5,
  "regionBoundary": {
    "x1": 53.76,
    "x2": 294.24,
    "y1": 82.56,
    "y2": 124.32
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Figure7-1.png"
}, {
  "caption": "Figure 1: A competitive programming problem excerpted from the CodeContest benchmark (#test87) [27]. Both the plans and codes are generated by GPT-3.5-Turbo.",
  "captionBoundary": {
    "x1": 53.79800033569336,
    "x2": 294.0330505371094,
    "y1": 305.53082275390625,
    "y2": 333.4649658203125
  },
  "figType": "Figure",
  "imageText": ["Failed", "Succeeded", "Don't", "put", "all", "eggs", "in", "one", "basket.", "We", "need", "backup", "plans!", "iterative", "repair", "...", "sequence", "not", "been", "updated,", "fix", "a[i]=i+1", "analysis", "repaired", "code", "...", "a[i]", "=", "i+1", "...", "Plan", "B", "...", "Iterate", "over", "the", "sequence:", "1.", "If", "ai", ">", "i+1,", "calculate", "the", "diff", ":", "ai", "-", "(i+1);", "2.", "keep", "track", "of", "the", "max", "diff", "so", "far;", "3.", "print", "the", "max", "diff", "as", "the", "result", "...", "Don't", "beat", "a", "dead", "horse!", "We", "need", "change", "another", "plan!", "...", "for", "i", "in", "range(n):", "max_diff", "=", "max(max_diff,", "\\", "a[i]", "-", "(i", "+", "1))", "print(max_diff)", "...", "Passed", "Plan", "A", "...", "Iterate", "over", "the", "sequence:", "1.", "If", "ai", ">", "i+1,", "calculate", "the", "diff", ":", "ai", "-", "(i+1);", "2.", "Increment", "operations", "by", "diff", "...", "...", "operations", "=", "0", "for", "i", "in", "range(n):", "if", "a[i]", ">", "i", "+", "1:", "operations", "+=", "a[i]", "-", "(i", "+", "1)", "...", "Wrong", "answer", "···", "...", "incorrectly", "calculates", "the", "difference", "between", "ai", "and", "(i", "+", "1)", "...", "...", "operations", "+=", "max(0,", "a[i]", "-", "(i", "+", "1))", "...", "Problem", "description", "Given", "an", "integer", "sequence", "a1,", "a2,", "…,", "an,", "you", "can", "perform", "the", "operation", "any", "number", "of", "times:", "*", "Select", "any", "positive", "integer", "k,", "then", "insert", "k", "into", "the", "sequence", "at", "any", "position.", "Find", "the", "minimum", "number", "of", "operations", "to", "make", "the", "sequence", "satisfy:", "ai", "≤", "i", "(1", "≤", "i", "≤", "n)."],
  "name": "1",
  "page": 1,
  "regionBoundary": {
    "x1": 52.8,
    "x2": 294.24,
    "y1": 84.0,
    "y2": 292.32
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Figure1-1.png"
}, {
  "caption": "Table 7: Analysis of passes on public test cases Tv , private test cases Th , and specific error types on Th , i.e., Runtime Error (RE), Wrong Answer (WA), and Time Limit Exceeded (TLE).",
  "captionBoundary": {
    "x1": 317.65899658203125,
    "x2": 558.2007446289062,
    "y1": 85.34080505371094,
    "y2": 124.23394775390625
  },
  "figType": "Table",
  "imageText": ["HumanEval", "95.73%", "87.80%", "0", "11.59%", "0.61%", "MBPP", "93.20%", "80.60%", "0.80%", "18.60%", "0", "CodeContest-test", "21.21%", "15.15%", "16.36%", "60.00%", "8.48%", "RE", "WA", "TLE", "Benchmarks", "Pass", "Tv", "Pass", "Th", "Not", "pass", "Th"],
  "name": "7",
  "page": 9,
  "regionBoundary": {
    "x1": 322.56,
    "x2": 553.4399999999999,
    "y1": 138.72,
    "y2": 204.48
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table7-1.png"
}, {
  "caption": "Table 5: Accuracy comparison of prompting approaches and Self-repair usingGPT-3.5-Turbowith up to 10 iterations. HumanEval and MBPP are the original versions (the same below unless otherwise specified).",
  "captionBoundary": {
    "x1": 53.501983642578125,
    "x2": 295.6379089355469,
    "y1": 85.34080505371094,
    "y2": 124.23394775390625
  },
  "figType": "Table",
  "imageText": ["PairCoder", "87.80", "80.60", "15.15", "Direct", "prompting", "75.61", "72.80", "9.09", "CoT", "prompting", "76.83", "73.00", "7.88", "SCoT", "prompting", "75.61", "73.94", "8.48", "Self-planning", "79.27", "76.36", "10.90", "Self-repair", "78.65", "75.00", "9.09", "Approaches", "HumanEval", "MBPP", "CodeContest-test"],
  "name": "5",
  "page": 9,
  "regionBoundary": {
    "x1": 67.67999999999999,
    "x2": 280.32,
    "y1": 138.72,
    "y2": 224.16
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table5-1.png"
}, {
  "caption": "Table 6: Average number of API calls and tokens (in thousands) using GPT-3.5-Turbo. Note that all approaches are allowed up to 10 iterations.",
  "captionBoundary": {
    "x1": 53.50199890136719,
    "x2": 295.6378479003906,
    "y1": 242.0148162841797,
    "y2": 269.948974609375
  },
  "figType": "Table",
  "imageText": ["PairCoder", "5.28", "5.98", "5.37", "5.09", "17.56", "24.06", "Self-repair", "2.37", "2.40", "2.36", "2.02", "8.85", "19.82", "Reflexion", "8.47", "9.68", "9.58", "10.28", "-", "-", "LDB†", "-", "23", "-", "-", "-", "-", "Self-debugging", "2.74", "7.74", "2.55", "6.47", "9.07", "38.40", "INTERVENOR", "3.85", "2.05", "3.77", "1.83", "17.06", "13.72", "Direct", "prompting", "2.43", "1.19", "2.31", "1.23", "8.73", "9.79", "CoT", "prompting", "2.43", "1.36", "2.37", "1.41", "8.62", "10.16", "SCoT", "prompting", "4.63", "4.46", "4.61", "4.49", "17.30", "26.57", "Self-planning", "4.16", "3.27", "4.67", "2.78", "17.02", "20.03", "API", "Token", "API", "Token", "API", "Token", "Approaches", "HumanEval", "MBPP", "CodeContest-test"],
  "name": "6",
  "page": 9,
  "regionBoundary": {
    "x1": 52.8,
    "x2": 297.12,
    "y1": 284.64,
    "y2": 428.15999999999997
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table6-1.png"
}, {
  "caption": "Figure 2: Overview of our PairCoder, in which a Navigator agent and a Driver agent collaborate on code generation.",
  "captionBoundary": {
    "x1": 64.33000183105469,
    "x2": 547.6641235351562,
    "y1": 344.1828308105469,
    "y2": 350.1990051269531
  },
  "figType": "Figure",
  "imageText": ["▸", "Details", "▸", "Explanation", "▸", "In/Output", "format", "▸", "Edge", "cases", "▸", "…", "Reflection", "▸", "Pass", "Diverse", "plans", "···", "Navigatori", "t", "Private", "test", "cases", "Driver", "Historical", "memory", "▸", "Current", "code", "▸", "Execution", "feedback", "Repair", "strategy", "Repair", "current", "bug", "Change", "to", "another", "plan", "5.", "Direct", "next", "iteration", "Problem", "description", "Repaired", "code", "▸", "Problem———", "▸", "Code——", "▸", "Strategy———", "▸", "Runtime", "Error", "▸", "Wrong", "Answer", "▸", "Time", "Limit", "Exceeded", "6.", "Repair", "buggy", "code", "4.", "Perform", "code", "testing", "———", "▸", "Problem———", "▸", "Plan", "Executable", "code", "3.", "Generate", "initial", "code", "Remaining", "candidates", "▸", "Correctness", "▸", "Efficiency", "▸", "Robustness", "▸", "...", "2.", "Select", "optimal", "plan", "Representative", "plans", "Clustering", "Embedding", "1.", "Propose", "promising", "plans"],
  "name": "2",
  "page": 2,
  "regionBoundary": {
    "x1": 79.67999999999999,
    "x2": 533.28,
    "y1": 84.0,
    "y2": 330.24
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Figure2-1.png"
}, {
  "caption": "Table 3: Additional comparison of pass@1 with GPT-4 on the original HumanEval and the sanitized MBPP [4].",
  "captionBoundary": {
    "x1": 53.50199890136719,
    "x2": 294.0328369140625,
    "y1": 316.73883056640625,
    "y2": 333.7139892578125
  },
  "figType": "Table",
  "imageText": ["PairCoder", "93.90", "91.23", "MetaGPT†", "85.9", "87.7", "Reflexion†", "91.0", "77.1", "Self-collaboration†", "90.2", "78.9", "Direct", "prompting", "84.76", "82.71", "Approaches", "HumanEval", "MBPP"],
  "name": "3",
  "page": 7,
  "regionBoundary": {
    "x1": 97.92,
    "x2": 250.07999999999998,
    "y1": 348.47999999999996,
    "y2": 428.15999999999997
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table3-1.png"
}, {
  "caption": "Table 2: Comparison of pass@1 with two LLMs on code generation benchmarks. “†” denotes the value is directly cited from the respective original work, and “-” denotes the empty result due to reproducibility issues.",
  "captionBoundary": {
    "x1": 53.501983642578125,
    "x2": 558.1913452148438,
    "y1": 85.94279479980469,
    "y2": 102.91796875
  },
  "figType": "Table",
  "imageText": ["Relative", "improvement", "↑", "29.73%", "26.99%", "20.66%", "16.97%", "162.43%", "150.00%", "12.00%", "12.45%", "18.67%", "17.51%", "128.76%", "118.14%", "PairCoder", "(ours)", "87.80", "77.44", "80.60", "77.69", "17.95", "15.15", "85.37", "76.22", "78.80", "75.69", "13.68", "14.55", "Self-collaboration†", "74.40", "-", "68.20", "-", "-", "-", "-", "-", "-", "-", "-", "-", "Self-repair", "73.17", "65.24", "70.60", "69.42", "7.69", "6.67", "77.44", "70.12", "70.00", "70.43", "5.98", "7.27", "Self-debugging", "78.05", "72.56", "72.80", "70.43", "9.40", "11.52", "79.27", "73.78", "72.20", "71.12", "8.55", "13.33", "INTERVENOR", "77.44", "69.51", "73.40", "71.93", "8.55", "9.09", "79.88", "72.56", "72.60", "72.43", "7.69", "10.30", "Reflexion", "69.57", "-", "67.76", "-", "-", "-", "81.99", "-", "74.31", "-", "-", "-", "LDB†", "82.90", "-", "76.00", "-", "-", "-", "-", "-", "-", "-", "-", "-", "Direct", "prompting", "67.68", "60.98", "66.80", "66.42", "6.84", "6.06", "76.22", "67.78", "66.40", "64.41", "5.98", "6.67", "CoT", "prompting", "68.90", "62.80", "69.00", "67.17", "5.13", "5.45", "77.27", "68.90", "67.60", "67.67", "5.45", "6.67", "SCoT", "prompting", "68.29", "61.59", "62.60", "61.40", "5.98", "4.24", "73.17", "65.85", "61.80", "60.15", "4.27", "6.06", "Self-planning", "72.56", "64.63", "69.60", "67.67", "7.69", "6.06", "74.39", "68.90", "65.80", "68.17", "6.84", "9.09", "Orig", "Plus", "Orig", "Plus", "Valid", "Test", "Orig", "Plus", "Orig", "Plus", "Valid", "Test", "HumanEval", "MBPP", "CodeContest", "HumanEval", "MBPP", "CodeContest", "Approaches", "GPT-3.5-Turbo", "DeepSeek-Coder"],
  "name": "2",
  "page": 7,
  "regionBoundary": {
    "x1": 63.839999999999996,
    "x2": 548.16,
    "y1": 117.6,
    "y2": 300.0
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table2-1.png"
}, {
  "caption": "Figure 9: Accuracy changes with the maximum number of iterations, which evaluates PairCoder, Self-debugging, and INTERVENOR using GPT-3.5-Turbo on (a) HumanEval and (b) CodeContest-test benchmarks. The colored demarcations denote the average iteration counts at which PairCoder transitions to its next plan.",
  "captionBoundary": {
    "x1": 317.9549560546875,
    "x2": 559.80615234375,
    "y1": 431.99481201171875,
    "y2": 492.805908203125
  },
  "figType": "Figure",
  "imageText": ["(a)", "HumanEval", "(b)", "CodeContest-test", "4.5", "7.43.8", "6.7", "#", "iterations", "(%", ")", "@", "1", "pa", "ss", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "90", "85", "80", "75", "70", "65", "#", "iterations", "(%", ")", "@", "1", "pa", "ss", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "15", "13", "11", "9", "7", "5", "PairCoder", "Self-debugging", "INTERVENOR"],
  "name": "9",
  "page": 7,
  "regionBoundary": {
    "x1": 318.71999999999997,
    "x2": 559.1999999999999,
    "y1": 316.32,
    "y2": 417.59999999999997
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Figure9-1.png"
}, {
  "caption": "Table 4: Ablation results for multi-plan exploration (MP) and feedback-driven refinement (RF).",
  "captionBoundary": {
    "x1": 317.65899658203125,
    "x2": 558.754638671875,
    "y1": 85.34080505371094,
    "y2": 102.31597900390625
  },
  "figType": "Table",
  "imageText": ["DeepSeek", "-Coder", "PairCoder", "85.37", "76.22", "78.80", "75.69", "13.68", "14.55", "w/o", "MP", "78.86", "73.17", "73.40", "71.93", "9.40", "12.12", "w/o", "RF", "75.61", "69.51", "68.60", "67.92", "7.69", "10.91", "GPT-3.5", "-Turbo", "PairCoder", "87.80", "77.44", "80.60", "77.69", "17.95", "15.15", "w/o", "MP", "81.10", "73.78", "76.20", "71.68", "11.97", "10.91", "w/o", "RF", "74.39", "68.29", "72.20", "68.92", "8.55", "9.69", "Orig", "Plus", "Orig", "Plus", "Valid", "Test", "Models", "Variants", "HumanEval", "MBPP", "CodeContest"],
  "name": "4",
  "page": 8,
  "regionBoundary": {
    "x1": 318.71999999999997,
    "x2": 557.28,
    "y1": 116.64,
    "y2": 207.35999999999999
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Table4-1.png"
}, {
  "caption": "Figure 10: Accuracy changes with the number of clusters, which evaluates PairCoder by pass@1 score and the proportion of problems where all k plans are attempted.",
  "captionBoundary": {
    "x1": 53.44900131225586,
    "x2": 295.6462707519531,
    "y1": 206.4888153076172,
    "y2": 234.4229736328125
  },
  "figType": "Figure",
  "imageText": ["(a)", "HumanEval", "(b)", "CodeContest-test", "n", "(%", ")", "#", "cluster", "numbers", "k", "or", "tio", "pr", "op", "(%", ")", "@", "1", "pa", "ss", "1", "2", "3", "4", "5", "100", "80", "60", "40", "20", "0", "25", "20", "15", "10", "5", "#", "cluster", "numbers", "k", ")", "n", "(%", "or", "tio", "pr", "op", "(%", ")", "@", "1", "pa", "ss", "1", "2", "3", "4", "5", "100", "80", "60", "40", "20", "0", "95", "85", "75", "65", "proportion", "pass@1"],
  "name": "10",
  "page": 8,
  "regionBoundary": {
    "x1": 54.72,
    "x2": 293.28,
    "y1": 84.96,
    "y2": 192.48
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/kbse-ase2024/figures/10_1145-3691620_3695506-Figure10-1.png"
}]
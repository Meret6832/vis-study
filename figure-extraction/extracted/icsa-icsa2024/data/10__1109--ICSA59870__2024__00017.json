[{
  "caption": "Fig. 4: Model Architecture Decision",
  "captionBoundary": {
    "x1": 238.01365661621094,
    "x2": 377.793212890625,
    "y1": 263.80645751953125,
    "y2": 268.21728515625
  },
  "figType": "Figure",
  "imageText": ["«Force»", "Training", "Efﬁciency", "«Force»", "Resource", "Utilization", "«Force»", "Interpretability", "options", "«Force»", "Adaptability", "«uses»", "«uses»", "Reinforcement", "Learning", "«Practice»", "Multi-Agent", "«uses»", "Reinforcement", "Learning", "«Practice»", "Single-Agent", "*", "*", "*", "*", "«Context»", "Model", "**", "«Force»", "Scalability", "«Force»", "Complexity", "«Force»", "Modularity", "«Force»", "Performance", "«Practice»", "Market-Based", "Coordination", "Reward", "«Practice»", "Coordination", "via", "Shared", "«Practice»", "Hierarchical", "Models", "«Practice»", "Monolithic", "Model", "«Practice»", "Specialized", "Model", "with", "Coordinator", "Specialist", "Multi-Agent-Based", "Coordination", "«Practice»", "Specialized", "Models", "with", "forces", "context", "Option", "«Option»", "Model", "Architecture", "Decision", "«Decision»", "Model", "Architecture", "Force", "«Force»", "Model", "Architecture", "Context", "«Context»", "Model", "Architecture"],
  "name": "4",
  "page": 5,
  "regionBoundary": {
    "x1": 121.92,
    "x2": 494.4,
    "y1": 72.96,
    "y2": 255.35999999999999
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure4-1.png"
}, {
  "caption": "Fig. 5: Reinforcement Learning Model Training Decision",
  "captionBoundary": {
    "x1": 198.15391540527344,
    "x2": 419.0704650878906,
    "y1": 243.4526824951172,
    "y2": 247.863525390625
  },
  "figType": "Figure",
  "imageText": ["options", "*", "*", "«can", "use»", "«can", "be", "combined", "with»", "«can", "be", "combined", "with»", "*", "*", "*", "*", "a", "Single", "Agent", "«Practice»", "Parallel", "Training", "of", "«Force»", "Efﬁciency", "«Force»", "Complexity", "«Force»", "Training", "Efﬁciency", "Reinforcement", "Learning", "«Practice»", "Hierarchical", "«Context»", "RL", "Agent", "«Context»", "Model", "«Force»", "Scalability", "«Force»", "Utilization", "of", "Resources", "«Force»", "Performance", "«Practice»", "Distributed", "Multi-Agent", "Reinforcement", "Learning", "«Practice»", "Centralized", "Training", "with", "Centralized", "Execution", "«Practice»", "Centralized", "Training", "with", "Decentralized", "Execution", "Reinforcement", "Learning", "«Practice»", "Market-Based", "Learning", "(Market-Based", "MARL)", "«Practice»", "Single-Agent", "Reinforcement", "Learning", "«Practice»", "Multi-Agent", "forces", "contexts", "Model", "Training", "Option", "«Option»", "Reinforcement", "Learning", "«Decision»", "Reinforcement", "Learning", "Model", "Training", "Decision", "Model", "Training", "Force", "«Force»", "Reinforcement", "Learning", "«Context»", "Reinforcement", "Learning", "Model", "Training", "Context"],
  "name": "5",
  "page": 6,
  "regionBoundary": {
    "x1": 97.92,
    "x2": 519.36,
    "y1": 72.96,
    "y2": 234.23999999999998
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure5-1.png"
}, {
  "caption": "Fig. 6: Reinforcement Learning Checkpoints Decision",
  "captionBoundary": {
    "x1": 204.56851196289062,
    "x2": 412.6552734375,
    "y1": 361.8729553222656,
    "y2": 366.2837829589844
  },
  "figType": "Figure",
  "imageText": ["*", "*options", "*", "*", "*", "*", "«Force»", "Flexiblility", "«Force»", "Computation", "Overheads", "«Force»", "Efﬁciency", "«Context»", "RL", "Agent", "«Force»", "Reliablility", "«Option»", "No", "Use", "of", "Checkpoints", "«Context»", "Model", "«Force»", "Robustness", "«Force»", "Complexity", "Checkpoints", "«Option»", "Use", "of", "forces", "contexts", "Checkpoints", "Option", "«Option»", "Reinforcement", "Learning", "Decision", "Reinforcement", "Learning", "Checkpoints", "Checkpoints", "Force", "«Force»", "Reinforcement", "Learning", "Checkpoints", "Context", "«Context»", "Reinforcement", "Learning"],
  "name": "6",
  "page": 6,
  "regionBoundary": {
    "x1": 158.88,
    "x2": 459.35999999999996,
    "y1": 264.0,
    "y2": 353.28
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure6-1.png"
}, {
  "caption": "Fig. 1: Research Method Steps",
  "captionBoundary": {
    "x1": 372.6300354003906,
    "x2": 491.6103210449219,
    "y1": 125.66026306152344,
    "y2": 130.07110595703125
  },
  "figType": "Figure",
  "imageText": ["Model", "Constant", "Comparison", "Constant", "Comparison", "Constant", "Comparison", "Theoritical", "Saturation", "Selective", "Coding", "Axial", "Coding", "Open", "Coding", "Data", "Collection"],
  "name": "1",
  "page": 2,
  "regionBoundary": {
    "x1": 313.92,
    "x2": 548.16,
    "y1": 72.96,
    "y2": 115.67999999999999
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure1-1.png"
}, {
  "caption": "Fig. 7: Transfer Learning in Reinforcement Learning Decision",
  "captionBoundary": {
    "x1": 188.10870361328125,
    "x2": 428.41046142578125,
    "y1": 169.9390106201172,
    "y2": 174.349853515625
  },
  "figType": "Figure",
  "imageText": ["Resources", "«Force»", "Computational", "«Force»", "Resource", "Efﬁciency", "*", "*options", "*", "*", "*", "*", "«Force»", "Efﬁciency", "«Force»", "Generalization", "Transfer", "Learning", "«Option»", "No", "Use", "of", "«Force»", "Reliablility", "«Context»", "RL", "Agent", "«Context»", "Model", "«Force»", "Adaptability", "Transfer", "Learning", "«Option»", "Use", "of", "forces", "contexts", "Reinforcement", "Learning", "Option", "«Option»", "Transfer", "Learning", "in", "Decision", "Transfer", "Learning", "in", "Reinforcement", "Learning", "Reinforcement", "Learning", "Force", "«Force»", "Transfer", "Learning", "in", "Reinforcement", "Learning", "Context", "«Context»", "Transfer", "Learning", "in"],
  "name": "7",
  "page": 7,
  "regionBoundary": {
    "x1": 157.92,
    "x2": 458.4,
    "y1": 72.96,
    "y2": 161.28
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure7-1.png"
}, {
  "caption": "TABLE I: Knowledge Sources Included in the Study",
  "captionBoundary": {
    "x1": 207.336669921875,
    "x2": 411.24664306640625,
    "y1": 74.98533630371094,
    "y2": 79.39617919921875
  },
  "figType": "Table",
  "imageText": ["S1", "What", "is", "Model-Based", "Reinforcement", "Learning?", "https://bit.ly/3MNg3iR", "S2", "Reinforcement", "Learning", "Meets", "Large", "Language", "Models", "(LLMs):", "Aligning", "Human", "Preferences", "in", "LLMs", "https://bit.ly/47EbKP3", "S3", "What", "Is", "Better:", "One", "General", "Model", "or", "Many", "Specialized", "Models?", "https://bit.ly/47gHtpJ", "S4", "Multi-Agent", "Reinforcement", "Learning", "with", "Coordination", "Graphs", "https://bit.ly/3sAHkhA", "S5", "Model-Based", "RL", "for", "Decentralized", "Multi-agent", "Navigation", "https://bit.ly/3sF42VV", "S6", "EFlow", "—", "Racing", "towards", "millions", "of", "ML", "ﬂows", "https://bit.ly/49BARnj", "S7", "L33T", "M10y", "https://sforce.co/3MNfKom", "S8", "Generalists", "vs", "(Micro)Specialists", "in", "AI", "architectures", "https://bit.ly/49FxXhu", "S9", "Hierarchical", "Reinforcement", "Learning", "for", "Robustness,", "Performance", "and", "Explainability", "https://bit.ly/40CLNx3", "S10", "Hierarchical", "Reinforcement", "Learning", "https://bit.ly/3R17wLO", "S11", "Single", "agent", "vs", "multi", "agent", "system", "in", "AI", "https://bit.ly/40DwcNJ", "S12", "Train", "Agents", "Using", "Parallel", "Computing", "and", "GPUs", "https://bit.ly/49GQNVi", "S13", "Centralized", "Training", "and", "Decentralized", "Execution", "in", "Multi-Agent", "Reinforcement", "Learning", "https://shorturl.at/fpEN1", "S14", "Multi-Agent", "Reinforcement", "Learning", "(MARL)", "and", "Cooperative", "AI", "https://shorturl.at/bciM3", "S15", "Decentralized", "Multi-Agent", "Reinforcement", "Learning", "and", "Game", "Theory", "https://shorturl.at/fANQW", "S16", "What", "is", "Hierarchical", "Reinforcement", "Learning?", "https://shorturl.at/tGVW1", "S17", "The", "Promise", "of", "Hierarchical", "Reinforcement", "Learning", "https://shorturl.at/ekBN1", "S18", "Saving", "and", "Loading", "your", "RL", "Algorithms", "and", "Policies", "https://t.ly/8BaaS", "S19", "Running", "RLlib", "Experiments", "https://t.ly/DOnRn", "S20", "Accelerate", "Training", "in", "RL", "Using", "Distributed", "Reinforcement", "Learning", "Architectures", "https://t.ly/JmhJZ", "S21", "Parallelism", "Strategies", "for", "Distributed", "Training", "https://t.ly/Y9P7Q", "S22", "Deep", "Reinforcement", "Learning", "and", "Hyperparameter", "Tuning", "https://t.ly/FQe2V", "S23", "Hyperparameter", "Tuning", "in", "Reinforcement", "Learning", "is", "Easy,", "Actually", "https://rb.gy/3kj2hq", "S24", "Transfer", "Learning", "in", "Reinforcement", "Learning", "https://rb.gy/xi6q56", "S25", "RL", "—", "Transfer", "Learning", "https://rb.gy/a37rhn", "S26", "Introduction", "to", "Experience", "Replay", "for", "Off-Policy", "Deep", "Reinforcement", "Learning", "https://rb.gy/dh0l5d", "S27", "Understanding", "Gradient", "Clipping", "(and", "How", "It", "Can", "Fix", "Exploding", "Gradients", "Problem)", "https://rb.gy/2rdzuk", "S28", "How", "to", "Improve", "Your", "Network", "Performance", "by", "Using", "Curriculum", "Learning", "https://rb.gy/hnhh4d", "S29", "Curriculum", "Learning", "https://rb.gy/wgjs6p", "S30", "Parameter", "Sharing", "and", "Tying", "https://rb.gy/enucvc", "S31", "A", "Complete", "Guide", "to", "Data", "Augmentation", "https://rb.gy/so60h0", "S32", "Model", "Compression", "Techniques", "for", "Edge", "AI", "https://rb.gy/84ih5l", "S33", "Distributed", "training", "with", "TensorFlow", "https://urlis.net/myucb96s", "ID", "Description", "Reference"],
  "name": "I",
  "page": 3,
  "regionBoundary": {
    "x1": 70.56,
    "x2": 546.24,
    "y1": 87.84,
    "y2": 437.28
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-TableI-1.png"
}, {
  "caption": "Fig. 2: Meta-model for ADD Models",
  "captionBoundary": {
    "x1": 236.5282745361328,
    "x2": 380.1099548339844,
    "y1": 605.9762573242188,
    "y2": 610.3870849609375
  },
  "figType": "Figure",
  "imageText": ["PracticePattern", "has", "sub-category", "*", "Category", "Force", "*", "contexts", "options", "*", "*", "forces", "next", "decision", "relations", "*", "relations*", "next", "decision", "relations", "*", "source", "1", "target", "1", "RelationRelation", "Element", "**", "Context", "Solution", "Option", "description:", "String", "type:", "Named", "Element", "Type", "name:", "String", "Named", "Element", "Decision"],
  "name": "2",
  "page": 3,
  "regionBoundary": {
    "x1": 146.88,
    "x2": 464.15999999999997,
    "y1": 449.76,
    "y2": 597.12
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure2-1.png"
}, {
  "caption": "Fig. 8: Reinforcement Learning Distribution Strategy Decision",
  "captionBoundary": {
    "x1": 188.35011291503906,
    "x2": 429.0451354980469,
    "y1": 287.41375732421875,
    "y2": 291.8245849609375
  },
  "figType": "Figure",
  "imageText": ["typically", "realized", "with", "typically", "realized", "with", "typically", "realized", "with", "Task", "Distribution", "«Practice»", "Hierarchical", "Parallel", "«Practice»", "Logically", "Centralized", "Control", "Distributionn", "«Practice»", "Distributed", "Control-", "Based", "Distribution", "«Practice»", "Distribution", "Style", "Frameworks", "«Practice»", "Versatile", "Distribution", "«Practice»", "Custom", "Distribution", "Frameworks", "for", "Each", "RL", "Algorithm", "«Practice»", "Distribution", "Framework", "*", "*", "options", "*", "*", "*", "*", "«Force»", "Training", "Times", "«Force»", "Scalability", "Updates", "«Force»", "Synchronized", "Parameter", "Distribution", "Strategy", "«Option»", "No", "Use", "of", "«Context»", "RL", "Agent", "«Context»", "Model", "«Force»", "Resource", "Utilization", "Distribution", "Strategy", "«Option»", "Use", "of", "forces", "contexts", "Distribution", "Strategy", "Option", "«Option»", "Reinforcement", "Learning", "Decision", "Reinforcement", "Learning", "Distribution", "Strategy", "Distribution", "Strategy", "Force", "«Force»", "Reinforcement", "Learning", "Distribution", "Strategy", "Context", "«Context»", "Reinforcement", "Learning"],
  "name": "8",
  "page": 8,
  "regionBoundary": {
    "x1": 158.88,
    "x2": 459.35999999999996,
    "y1": 72.96,
    "y2": 275.03999999999996
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure8-1.png"
}, {
  "caption": "Fig. 9: Reinforcement Learning Hyperparameter Tuning Decision",
  "captionBoundary": {
    "x1": 182.68943786621094,
    "x2": 434.7066650390625,
    "y1": 411.4439392089844,
    "y2": 415.8547668457031
  },
  "figType": "Figure",
  "imageText": ["options", "«Force»", "Training", "Efﬁciency", "*", "*", "*", "*", "**", "«Force»", "Resource", "Utilization", "«Force»", "Performance", "Hyperparameter", "Tuning", "«Option»", "No", "Use", "of", "«Context»", "RL", "Agent", "«Context»", "Model", "«Force»", "Resource", "Allocation", "Hyperparameter", "Tuning", "«Option»", "Use", "of", "forces", "contexts", "Option", "«Option»", "Reinforcement", "Learning", "Hyperparameter", "Tuning", "Decision", "Reinforcement", "Learning", "Hyperparameter", "Tuning", "Force", "«Force»", "Reinforcement", "Learning", "Hyperparameter", "Tuning", "Context", "«Context»", "Reinforcement", "Learning", "Hyperparameter", "Tuning"],
  "name": "9",
  "page": 8,
  "regionBoundary": {
    "x1": 158.88,
    "x2": 459.35999999999996,
    "y1": 307.68,
    "y2": 402.24
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure9-1.png"
}, {
  "caption": "Fig. 10: Number of Elements of Newly-Added Sources",
  "captionBoundary": {
    "x1": 202.01771545410156,
    "x2": 415.3768310546875,
    "y1": 615.6429443359375,
    "y2": 620.0537719726562
  },
  "figType": "Figure",
  "imageText": [],
  "name": "10",
  "page": 8,
  "regionBoundary": {
    "x1": 140.64,
    "x2": 476.15999999999997,
    "y1": 432.0,
    "y2": 607.1999999999999
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure10-1.png"
}, {
  "caption": "Fig. 3: Reusable ADD Model on Training Strategies in Reinforcement Learning Architectures: Overview",
  "captionBoundary": {
    "x1": 105.90948486328125,
    "x2": 510.8638000488281,
    "y1": 219.7393035888672,
    "y2": 224.150146484375
  },
  "figType": "Figure",
  "imageText": ["«Optional", "Next»", "«Optional", "Next»", "«Optional", "Next»", "«Optional", "Next»", "«Optional", "Next»", "Reinforcement", "Learning", "Hyperparameter", "Tuning", "1..*", "decisions", "Transfer", "Learning", "in", "Reinforcement", "Learning", "Reinforcement", "Learning", "Checkpoints", "Model", "Architecture", "Reinforcement", "Learning", "Distribution", "Strategy", "Reinforcement", "Learning", "Model", "Training", "Training", "in", "RL", "Decision", "Training", "Efﬁciency", "and", "Optimization", "Strategies", "in", "RL", "Category"],
  "name": "3",
  "page": 4,
  "regionBoundary": {
    "x1": 172.79999999999998,
    "x2": 446.4,
    "y1": 72.96,
    "y2": 207.35999999999999
  },
  "renderDpi": 150,
  "renderURL": "../datacollection/pdf_analysis/icsa-icsa2024/figures/10_1109-ICSA59870_2024_00017-Figure3-1.png"
}]